논문 제목 : “BERTRAM: Improved Word Embeddings Have Big Impact on Contextualized Model Performance”
						url: https://aclanthology.org/2020.acl-main.368/

논문을 선택한 이유는 자연어처리에서 	acl은 높은 수준의 학회이며, 그 중 특히 선택한 논문의 내용 또한 흥미로워서 선택하였습니다.	
	
	>>>Summary<<<

 이 논문은 기존 Contextualized Embedding을 얻는 방식(Bert, RoBert)에서 희귀한 단어에 취약하다는 문제점을 다루고 있습니다. 이러한 문제를 해결하기 위해 정적 임베딩이 아닌 논문에서 제시한 BERTRAM 방식 활용한 추가 임베딩을 만드는 것입니다. 희귀한 단어를 위해 임베딩을 새로 만들고, 형태적인 표현을 만들어 사전 훈련된 Bert 모델과 Attention Mimicking(AM) 기법을 결합한 희귀 단어 표현 학습을 진행합니다. 단어의 여러 문맥들을 활용하기 위해 ‘Schick and Schutz’의 접근 방식을 따르고 모델 위에 AM 계층을 추가하는 구조이다. 이러한 구조에서 단어의 표면 형태(형식에 대한 모든 것)와 문맥이 서로 상호작용할 수 있기 때문에 효과적인 것으로 판단하고 있습니다. 

  BERTRAM은 single context, multiple contexts과정에선 문맥 ‘C’에 대해서 다르게 정의한다. 단일의 경우에는 문장에서 나온 대체할 수 있는 단어들로 구성하며, multiple contexts의 경우에는 문장의 대표되는 대체 단어들을 여러 문맥에서 찾아서 구성합니다.

  Training 과정에서 3단계로 나눠서 진행하며, end-to-end로 훈련하는 것은 cost가 높기 때문에 논문에서는 Bert의 모든 매개 변수를 동결하는 방법을 사용합니다. 이러한 방법은 훈련 속도 증가 및 모델 성능 향상에 도움을 줄 수 있다고 증명했습니다. 모든 그레디언트를 얻기 위해 BERT를 통해 역진파(backpropagate)할 필요가 없어져서 훈련 속도가 증가한 것이라고 추가 설명했습니다.

  데이터 세트 희소화 과정에선 우선, 베이스라인 모델을 훈련합니다. 그 이후 정확하게 분류하는 모델 샘플을 선택합니다. 이 후 Mask 토큰으로 재배치될 때 모델이 예측을 바꾸는 단어들을 찾는다. 마지막으로      식별된 단어를 희귀 동의어로 대체합니다. 이 때 BERTRAM의 세 가지 변형인 BERTRAM-Shallow, BERTRAM Replace, BERTRAM-ADD로 실험을 진행하였고, BERTRAM-ADD가 전반적으로 가장 좋은 결과를 나타냈습니다.
위의 결과에서 주목해서 봐야할 부분은 각 데이터 셋과 BERT기반, RoBERTa기반 모델 중에서 희귀 단어 카운트의 모든 단일 간격에 대해 BERTRAM-SLASH를 추가하면 정확도가 가장 많이 올라간다는 사실을 알 수 있다.또한 논문에서 BERT에 BERTRAM을 추가하면 WNLaM Pro에서 우수한 결과를 가져오고, 세 개의 데이터 세트(MNLI, AGNews, DBPedia)에서 모두 기존 방법을 능가하는 연구 결과를 제시함으로써 연구의 타당성을 증명하였다. 또한 연구는 BERTRAM을 추가하는 것이 희귀 단어뿐만 아니라, 빈번하게 출현하는 단어들에 대해서도 효과가 있다고 증명했습니다. 단, RoBERTa의 경우에는 BERTRAM 추가할 경우 500회 미만의 단어만 개선되고, 그 이 후부터는 하락하는 것을 알 수 있습니다.

		>>>Evaluation<<<

논문이 좋았던 점: 
자연어처리에서 가장 중요하게 여겨지는 부분은 완벽한 번역 및 대답 추출이라고 생각합니다. 완벽한 것은 없을 수 있지만, 어느정도의 해결을 위해 빈도수가 낮은 단어에 대한 분류 및 해결에 대한 문제는 앞으로도 계속 해결하고자 할 것입니다. 이 논문에선 ’희귀 단어’를 정확하게 예측하는 난제를 해결하기 위해서 기존 방식에서 추가하여 더 나은 표현을 가능하게 하였다. 많이 나온 단어를 예측하는 문제도 어렵지만, 희귀 단어 어떻게 보면 이상치같은 값을 잘 처리하는 방식도 중요합니다. 외국에서 바나나를 분류할 때 과일인 것을 모두 알지만, 예를 들어서 자주 쓰이지 않는 단어(금귤)을 과일로 분류하지 못하는 경우가 많을 수 있다. 그렇기에 BERTRAM을 이용해 해결하고자 하는 이 논문의 문제는 의미가 있고, 가치있다고 판단합니다. 
이해를 돕기 위해서 논문은 기존 데이트 세트를 이용하면 안되는 이유를 서술하여 더욱 이해를 도왔으며, 맥락이라는 키워드를 계속 반복시켜서 말하고자 하는 것을 정확하게 전달하고자 했다고 느꼈습니다. 
논문 평가 부분에서는 모델 선정하는 과정에서의 실험 결과와 단일 데이터 세트를 이용한 것이 아니라 세 개 모두 동일하게 우수하다는 사실을 증명하면서 설득력있는 결과를 이끌었습니다. 

논문에 대한 의문점(weakness):
희귀 단어라는 정의가 모호한 것 같다. 
사실 희귀 단어라는 것은 미리 저장된 데이터세트의 비중이 적은 단어들을 말할 수 있다. 하지만, 정보가 많이 없는 단어일 수도 있다. 예를 들어서, 새롭게 추가된 은어, 줄임말 같은 것들은 횟수는 많으나, 되게 희귀 단어라고 판단할 수 있다. 하지만, 이 논문을 쓰기엔 언급이 많지않아서 정보가 부족하고, 표현하기 어렵다고 생각합니다.
한국어같은 경우 접사 추가에 따른 다른 의미 발생이라는 것이 생길 수 있고, 모호한 띄어쓰기 규칙에 의한 다른 단어의 완성이 될 수 있다고 생각합니다. 이 논문에서는 영어의 기준이겠지만, 한국어 같은 경우에는 아예 다른 방식인지 의문이 들었습니다.

비교 대상에 대한 의문
희귀 단어에 대한 처리를 위해서 쓰이는 방법 중에서 가장 효율적이라는 것을 의미하는 것인지, 현재 NLP영역의 발전을 가져왔던 BERT 기법에서 문제점이 있고 그걸 해결하기 위해서는 BERTRAM을 이용해서 해결할 수 있다는 것인지 이 부분이 헷갈립니다.
